{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Jkvj2sTwVil",
        "outputId": "9b3a2fc6-3873-4825-e3e6-c66d952ff227"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Libraries\n",
        "!pip install -q langchain torch transformers sentence-transformers datasets faiss-cpu unstructured chromadb gradio"
      ],
      "metadata": {
        "id": "ae7awZaMwRK5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.vectorstores import FAISS\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "3_ED7HmKxk1p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Documents from Google Drive\n",
        "loader = DirectoryLoader('/content/drive/MyDrive/preprocessed_fyp')\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "L65_BrEsx5Vo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Documents into Chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        ")\n",
        "documents = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "SBSAVj7Cx5TH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Embedding Model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKURCysfx5Qu",
        "outputId": "00caeb7d-ae53-4507-f82b-908353d56da6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Vector Store with Chroma\n",
        "db = FAISS.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "id": "imTjvB-Hx5Na"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Similarity Search\n",
        "question = \"What is chemical engineering?\"\n",
        "search_docs = db.similarity_search(question)\n",
        "print(f\"Sample Search Result: {search_docs[0].page_content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq1ion7zx5KK",
        "outputId": "4793f7c1-34ff-4c90-bad0-a25754745f18"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Search Result: measurable way there may be more than one measurable specific objectives defined for a given competency the specific objectives identified for chemical engineering program are presented below 1 apply knowledge of basic science engineering fundamentals and chemical engineering principles to solve complex engineering problems 2 identify formulate and analyze complex chemical engineering problems to design their solutions develop systems and processes that meet specified needs with appropriate consideration for public health safety and environmental considerations using principles of science and chemical engineering 3 demonstrate knowledge and understanding of engineering management principles and economic decisionmaking and apply them to manage projects in multidisciplinary environments 4 recognize the need for and have the preparation and ability to engage in independent and lifelong learning in the broadest context of technological changes in chemical engineering profession 6 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the LLM Model (distilGPT2)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n"
      ],
      "metadata": {
        "id": "nyAAKIe1xnAA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Text Generation Pipeline with `max_new_tokens`\n",
        "text_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=1024,  # Adjusted max_length\n",
        "    max_new_tokens=200  # Controls the number of tokens generated\n",
        ")"
      ],
      "metadata": {
        "id": "VuP-lUtfxm9y"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create HuggingFacePipeline LLM\n",
        "llm = HuggingFacePipeline(pipeline=text_generator)"
      ],
      "metadata": {
        "id": "qHqck093xm7h"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Retriever\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 2})"
      ],
      "metadata": {
        "id": "HKpMhdrIxm4v"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the RetrievalQA Chain\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"refine\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "Wbf4wwozxm2B"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Query the Chatbot\n",
        "def query_chatbot(question: str) -> str:\n",
        "    try:\n",
        "        result = qa.run(question)\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\""
      ],
      "metadata": {
        "id": "b_93EcI4xmyu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio Interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# AASTU_Chat.AI\")\n",
        "    question_input = gr.Textbox(label=\"Ask a Question\")\n",
        "    answer_output = gr.Textbox(label=\"Answer\", interactive=False)\n",
        "    submit_button = gr.Button(\"Submit\")\n",
        "    submit_button.click(fn=query_chatbot, inputs=question_input, outputs=answer_output)\n",
        "\n",
        "# Launch Gradio App\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "9dRl8zN5xmvO",
        "outputId": "efe6e1f5-e426-41eb-97a2-c460eab5391b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://942d0e974c23d0c525.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://942d0e974c23d0c525.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lBHWKkwMXgjN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**With Prompt Template**"
      ],
      "metadata": {
        "id": "Sh3T-q3KXqhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Install the Libraries\n",
        "# !pip install -q langchain torch transformers sentence-transformers datasets faiss-cpu unstructured chromadb gradio\n",
        "\n",
        "# # Import Libraries\n",
        "# from langchain.document_loaders import DirectoryLoader\n",
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# from langchain.embeddings import HuggingFaceEmbeddings\n",
        "# from langchain.vectorstores import Chroma\n",
        "# from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
        "# from langchain import HuggingFacePipeline, LLMChain\n",
        "# from langchain.chains import RetrievalQA, StuffDocumentsChain\n",
        "# from langchain.prompts import PromptTemplate\n",
        "# import gradio as gr\n",
        "\n",
        "# # Load Documents from Google Drive\n",
        "# loader = DirectoryLoader('/content/drive/MyDrive/preprocessed_fyp')\n",
        "# data = loader.load()\n",
        "\n",
        "# # Split Documents into Chunks\n",
        "# text_splitter = RecursiveCharacterTextSplitter(\n",
        "#     chunk_size=1000,\n",
        "#     chunk_overlap=200,\n",
        "#     separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        "# )\n",
        "# documents = text_splitter.split_documents(data)\n",
        "\n",
        "# # Text Embedding Model\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# # Create the Vector Store with Chroma\n",
        "# db = Chroma.from_documents(documents, embeddings)\n",
        "\n",
        "# # Test Similarity Search\n",
        "# question = \"What is chemical engineering?\"\n",
        "# search_docs = db.similarity_search(question)\n",
        "# print(f\"Sample Search Result: {search_docs[0].page_content}\")\n",
        "\n",
        "# # Prepare the LLM Model (distilGPT2)\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
        "# model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
        "\n",
        "# # Create Text Generation Pipeline with `max_new_tokens`\n",
        "# text_generator = pipeline(\n",
        "#     \"text-generation\",\n",
        "#     model=model,\n",
        "#     tokenizer=tokenizer,\n",
        "#     max_length=1024,  # Adjusted max_length\n",
        "#     max_new_tokens=200  # Controls the number of tokens generated\n",
        "# )\n",
        "\n",
        "# # Create HuggingFacePipeline LLM\n",
        "# llm = HuggingFacePipeline(pipeline=text_generator)\n",
        "\n",
        "# # Create Retriever\n",
        "# retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "# # Define the Prompt Template\n",
        "# prompt_template = \"\"\"\n",
        "# Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "# {context}\n",
        "\n",
        "# Question: {question}\n",
        "# Helpful Answer:\n",
        "# \"\"\"\n",
        "\n",
        "# prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "# # Create the LLM Chain\n",
        "# llm_chain = LLMChain(\n",
        "#     llm=llm,\n",
        "#     prompt=prompt\n",
        "# )\n",
        "\n",
        "# # Create the StuffDocumentsChain\n",
        "# stuff_docs_chain = StuffDocumentsChain(\n",
        "#     llm_chain=llm_chain\n",
        "# )\n",
        "\n",
        "# # Combine into RetrievalQA Chain\n",
        "# qa = RetrievalQA(\n",
        "#     retriever=retriever,\n",
        "#     combine_documents_chain=stuff_docs_chain\n",
        "# )\n",
        "\n",
        "# # Function to Query the Chatbot\n",
        "# def query_chatbot(question: str) -> str:\n",
        "#     try:\n",
        "#         result = qa.run(question)\n",
        "#         return result\n",
        "#     except Exception as e:\n",
        "#         return f\"An error occurred: {str(e)}\"\n",
        "\n",
        "# # Gradio Interface\n",
        "# with gr.Blocks() as demo:\n",
        "#     gr.Markdown(\"# Retrieval-Augmented Generation (RAG) Chatbot\")\n",
        "#     question_input = gr.Textbox(label=\"Ask a Question\")\n",
        "#     answer_output = gr.Textbox(label=\"Answer\", interactive=False)\n",
        "#     submit_button = gr.Button(\"Submit\")\n",
        "#     submit_button.click(fn=query_chatbot, inputs=question_input, outputs=answer_output)\n",
        "\n",
        "# # Launch Gradio App\n",
        "# demo.launch(share=True)"
      ],
      "metadata": {
        "id": "4pBLSZkd6LqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**With Prompt**"
      ],
      "metadata": {
        "id": "KiwYPJPOioV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Install the Libraries\n",
        "# !pip install -q langchain torch transformers sentence-transformers datasets faiss-cpu unstructured chromadb gradio\n",
        "\n",
        "# # Import Libraries\n",
        "# from langchain.document_loaders import DirectoryLoader\n",
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# from langchain.embeddings import HuggingFaceEmbeddings\n",
        "# from langchain.vectorstores import Chroma\n",
        "# from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
        "# from langchain import HuggingFacePipeline\n",
        "# from langchain.chains import RetrievalQA\n",
        "# import gradio as gr\n",
        "\n",
        "# # Load Documents from Google Drive\n",
        "# loader = DirectoryLoader('/content/drive/MyDrive/preprocessed_fyp')\n",
        "# data = loader.load()\n",
        "\n",
        "# # Split Documents into Chunks\n",
        "# text_splitter = RecursiveCharacterTextSplitter(\n",
        "#     chunk_size=1000,\n",
        "#     chunk_overlap=200,\n",
        "#     separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        "# )\n",
        "# documents = text_splitter.split_documents(data)\n",
        "\n",
        "# # Text Embedding Model\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# # Create the Vector Store with Chroma\n",
        "# db = Chroma.from_documents(documents, embeddings)\n",
        "\n",
        "# # Test Similarity Search\n",
        "# question = \"What is chemical engineering?\"\n",
        "# search_docs = db.similarity_search(question)\n",
        "# print(f\"Sample Search Result: {search_docs[0].page_content}\")\n",
        "\n",
        "# # Prepare the LLM Model (distilGPT2)\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
        "# model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
        "\n",
        "# # Create Text Generation Pipeline with `max_new_tokens`\n",
        "# text_generator = pipeline(\n",
        "#     \"text-generation\",\n",
        "#     model=model,\n",
        "#     tokenizer=tokenizer,\n",
        "#     max_length=1024,  # Adjusted max_length\n",
        "#     max_new_tokens=200  # Controls the number of tokens generated\n",
        "# )\n",
        "\n",
        "# # Create HuggingFacePipeline LLM\n",
        "# llm = HuggingFacePipeline(pipeline=text_generator)\n",
        "\n",
        "# # Create Retriever\n",
        "# retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "# # Setup the RetrievalQA Chain\n",
        "# qa = RetrievalQA.from_chain_type(\n",
        "#     llm=llm,\n",
        "#     chain_type=\"stuff\",\n",
        "#     retriever=retriever,\n",
        "#     return_source_documents=False\n",
        "# )\n",
        "\n",
        "# # Function to Query the Chatbot\n",
        "# def query_chatbot(question: str) -> str:\n",
        "#     try:\n",
        "#         result = qa.run(question)\n",
        "#         return result\n",
        "#     except Exception as e:\n",
        "#         return f\"An error occurred: {str(e)}\"\n",
        "\n",
        "# # Gradio Interface\n",
        "# with gr.Blocks() as demo:\n",
        "#     gr.Markdown(\"# Retrieval-Augmented Generation (RAG) Chatbot\")\n",
        "#     question_input = gr.Textbox(label=\"Ask a Question\")\n",
        "#     answer_output = gr.Textbox(label=\"Answer\", interactive=False)\n",
        "#     submit_button = gr.Button(\"Submit\")\n",
        "#     submit_button.click(fn=query_chatbot, inputs=question_input, outputs=answer_output)\n",
        "\n",
        "# # Launch Gradio App\n",
        "# demo.launch(share=True)"
      ],
      "metadata": {
        "id": "cAe31mS0inAY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}